{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFaSyxwHvKRc"
   },
   "source": [
    "# I. Set ups for Data\n",
    "- Create data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xurTLQvkvTYW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4f1ebd5990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "#device agnostic\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#Hyper Parameters\n",
    "RAND_SEED = 420\n",
    "torch.manual_seed(RAND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLs_tsjKv3I2"
   },
   "source": [
    "## 1. Data Handling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd6yws-Av7Js"
   },
   "source": [
    "### 1.1 Set up for data handling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIjVX1DLwKFo",
    "outputId": "e898e1c5-62e8-44c6-b3bb-c76d99ba7442"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "#Data paths\n",
    "data_folder_path = Path(\"dataset\")\n",
    "data_path = data_folder_path / \"cylinder_flow\"\n",
    "train_path = data_path / \"train\"\n",
    "test_path = data_path / \"test\"\n",
    "dev_path = data_path / \"valid\"\n",
    "\n",
    "CHANNELS =2 #(vx,vy)\n",
    "GRID_SIZE =64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Datasets classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sol_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# check if its np arr\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43msol_tensor\u001b[49m,np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversted sol solution to np array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m   sol_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sol_tensor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sol_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "# check if its np arr\n",
    "if not isinstance(sol_tensor,np.ndarray):\n",
    "  print(\"Conversted sol solution to np array\")\n",
    "  sol_tensor = np.array(sol_tensor)\n",
    "\n",
    "class BurgerDataset_Autoregressive_Sparse(Dataset):\n",
    "    \"\"\"\n",
    "    A *faster* sliding window dataset (e.g., 10-in, 1-out).\n",
    "    It creates a smaller, randomized dataset of (N_sims * windows_per_sim) samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_tensor, t_in=10, t_out=1, windows_per_sim=20):\n",
    "        self.data = data_tensor # Shape: (Num_Sims, time_step, num_nodes, 2)\n",
    "        self.t_in = t_in\n",
    "        self.t_out = t_out\n",
    "        self.total_steps = t_in + t_out \n",
    "        \n",
    "        # Max possible start time\n",
    "        self.max_start_time = self.data.shape[1] - self.total_steps \n",
    "        \n",
    "        self.sims_in_dataset = self.data.shape[0]\n",
    "        self.windows_per_sim = windows_per_sim\n",
    "        \n",
    "        # Total length of the dataset is now much smaller\n",
    "        self.length = self.sims_in_dataset * self.windows_per_sim\n",
    "        # e.g., 1000 sims * 20 windows/sim = 20,000 samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Figure out which simulation this index belongs to\n",
    "        sim_index = idx // self.windows_per_sim\n",
    "        \n",
    "        # 2. Pick a RANDOM start time for the window\n",
    "        # This makes the dataset different every epoch, which is good for training\n",
    "        start_time_index = np.random.randint(0, self.max_start_time + 1)\n",
    "        \n",
    "        # 3. Define the end of the input and output slices\n",
    "        end_in_index = start_time_index + self.t_in   # e.g., 50 + 10 = 60\n",
    "        end_out_index = end_in_index + self.t_out    # e.g., 60 + 1 = 61\n",
    "\n",
    "        # 4. Get the data chunks\n",
    "        x_np = self.data[sim_index, start_time_index : end_in_index, :,:].astype(np.float32) # Shape (t_in, num_nodes,2 )\n",
    "        y_np = self.data[sim_index, end_in_index : end_out_index, :,:].astype(np.float32)   # Shape (t_out,num_node,2)\n",
    "        \n",
    "        # 5. Transpose to (Points, Channels/Time) format\n",
    "        x = x_np.transpose(1, 0) # Shape (1024, 10)\n",
    "        y = y_np.transpose(1, 0) # Shape (1024, 1)\n",
    "        \n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "print(\"BurgerDataset_Autoregressive_Sparse (FASTER dataset) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BurgerDataset_Autoregressive_Sparse (FASTER dataset) defined.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class CylinderFlow_Autoregressive_Sparse(Dataset):\n",
    "    \"\"\"\n",
    "    A *faster* sliding window dataset (e.g., 10-in, 1-out).\n",
    "    It creates a smaller, randomized dataset of (N_sims * windows_per_sim) samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, t_in=10, t_out=1, windows_per_sim=20):\n",
    "        self.t_in = t_in\n",
    "        self.t_out = t_out\n",
    "        self.total_steps = t_in + t_out \n",
    "        self.data_dir = data_dir \n",
    "        self.file_paths = [os.path.join(self.data_dir,f) for f in os.listdir(self.data_dir)]\n",
    "        self.example = torch.load(self.file_paths[0])\n",
    "\n",
    "        \n",
    "        # Max possible start time\n",
    "        self.max_start_time = self.example[\"velocity\"].shape[0]- self.total_steps\n",
    "        self.sims_in_dataset = len(self.file_paths)\n",
    "        self.windows_per_sim = windows_per_sim\n",
    "\n",
    "        \n",
    "        # Total length of the dataset is now much smaller\n",
    "        self.length = self.sims_in_dataset * self.windows_per_sim\n",
    "        # e.g., 1000 sims * 20 windows/sim = 20,000 samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Figure out which simulation this index belongs to\n",
    "        sim_idx = idx // self.windows_per_sim\n",
    "        file_path = self.file_paths[sim_idx]\n",
    "        data = torch.load(file_path)\n",
    "        velocity_traj = data[\"velocity\"]\n",
    "        \n",
    "        # 2. Pick a RANDOM start time for the window\n",
    "        # This makes the dataset different every epoch, which is good for training\n",
    "        start_time_index = np.random.randint(0, self.max_start_time + 1)\n",
    "        \n",
    "        # 3. Define the end of the input and output slices\n",
    "        end_in_index = start_time_index + self.t_in   # e.g., 50 + 10 = 60\n",
    "        end_out_index = end_in_index + self.t_out    # e.g., 60 + 1 = 61\n",
    "\n",
    "        # 4. Get the data chunks\n",
    "        x_np = velocity_traj[start_time_index : end_in_index].to(torch.float32) # Shape (t_in, num_nodes,2 )\n",
    "        y_np = velocity_traj[end_in_index : end_out_index].to(torch.float32)   # Shape (t_out,num_node,2)\n",
    "        \n",
    "        # Input: (10, 64, 64, 2) -> (64, 64, 10, 2) -> (64, 64, 20)\n",
    "        x_tensor = x_np.permute(1, 2, 0, 3) \n",
    "        x_tensor = x_tensor.reshape(GRID_SIZE, GRID_SIZE, self.t_in * CHANNELS) \n",
    "        \n",
    "        # Output: (5, 64, 64, 2) -> (64, 64, 5, 2) -> (64, 64, 10)\n",
    "        y_tensor = y_np.permute(1, 2, 0, 3)\n",
    "        y_tensor = y_tensor.reshape(GRID_SIZE, GRID_SIZE, self.t_out * CHANNELS)\n",
    "        \n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "print(\"BurgerDataset_Autoregressive_Sparse (FASTER dataset) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bxCbENcSXxn"
   },
   "source": [
    "# II. Set up for model\n",
    "- Declare up model class\n",
    "- Set up loss functions\n",
    "- Train - test function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvlONEUK9rrM"
   },
   "source": [
    "## 1.Models set ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJtxm1_c9pux"
   },
   "source": [
    "### 1.1 FNO2D layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rwCocGjBmd-",
    "outputId": "45f5b864-143d-41c6-a79a-c73ec5b572b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNO_Layer2D class defined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class FNO_Layer2D(nn.Module):\n",
    "  \"\"\" A class act as one Fourier Layer as described in the original paper\"\"\"\n",
    "  def __init__(self,in_chanels, out_chanels,mode_x,mode_y):\n",
    "    super(FNO_Layer2D, self).__init__()\n",
    "    self.in_chanels = in_chanels\n",
    "    self.out_chanels = out_chanels\n",
    "    self.mode_x = mode_x\n",
    "    self.mode_y = mode_y\n",
    "    #Scaler to scale the parameters\n",
    "    self.scalar = (1/(in_chanels * out_chanels))\n",
    "    self.weight_x = nn.Parameter(\n",
    "        self.scalar * torch.rand(in_chanels,out_chanels,mode_x, mode_y, dtype = torch.cfloat)\n",
    "    )\n",
    "    self.weight_y = nn.Parameter(\n",
    "        self.scalar * torch.rand(in_chanels,out_chanels,mode_x, mode_y,dtype = torch.cfloat)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    #Size \n",
    "    batch,_,H,W = x.shape\n",
    "      \n",
    "    # FFT (2D fourier transform)\n",
    "    x_ft = torch.fft.rfft2(x)\n",
    "      \n",
    "    ## Fourier layer\n",
    "    out_ft = torch.zeros(\n",
    "        batch, self.out_chanels,H, W //2 +1,\n",
    "        dtype = torch.cfloat,\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    out_ft[:,:,:self.mode_x,:self.mode_y] = torch.einsum(\n",
    "        \"bixy,ioxy->boxy\",\n",
    "        x_ft[:,:,:self.mode_x,:self.mode_y],\n",
    "        self.weight_x\n",
    "    )\n",
    "\n",
    "    out_ft[:,:,-self.mode_x:,:self.mode_y] = torch.einsum(\n",
    "        \"bixy,ioxy->boxy\",\n",
    "        x_ft[:,:,-self.mode_x:, :self.mode_y],\n",
    "        self.weight_y\n",
    "    )\n",
    "    ## Inverse FFT\n",
    "    x = torch.fft.irfft2(out_ft , s=(H,W))\n",
    "    return x\n",
    "print(\"FNO_Layer2D class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ8niYFl9tQW"
   },
   "source": [
    "### 1.2 FNO2D class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrdAHRpJHZ9c",
    "outputId": "fd2c80f3-ef01-4e53-fa5c-34e95a783416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNO2D class defined.\n"
     ]
    }
   ],
   "source": [
    "class FNO2D(nn.Module):\n",
    "  def __init__(self ,width, mode_x, mode_y,in_chanels =1, out_chanels =200) :\n",
    "    \"\"\" Default is 1 time step -> 200 timestep rest\"\"\"\n",
    "    super(FNO2D,self).__init__()\n",
    "    self.in_chanels = in_chanels + 2 # add one more for the grid\n",
    "    self.out_chanels = out_chanels\n",
    "    self.width = width\n",
    "    self.mode_x = mode_x\n",
    "    self.mode_y = mode_y\n",
    "      \n",
    "    # P layer ( lift the input chanel up):\n",
    "    self.fc0 = nn.Linear(self.in_chanels, self.width)\n",
    "\n",
    "    # T block: 4 Fourier layers\n",
    "    self.conv0 = FNO_Layer2D(width,width,mode_x,mode_y)\n",
    "    self.w0 = nn.Conv2d(self.width,self.width,1)\n",
    "\n",
    "    self.conv1 = FNO_Layer2D(width,width,mode_x,mode_y)\n",
    "    self.w1 = nn.Conv2d(self.width,self.width,1)\n",
    "\n",
    "    self.conv2 = FNO_Layer2D(width,width,mode_x,mode_y)\n",
    "    self.w2 = nn.Conv2d(self.width,self.width,1)\n",
    "\n",
    "    self.conv3 = FNO_Layer2D(width,width,mode_x,mode_y)\n",
    "    self.w3 = nn.Conv2d(self.width,self.width,1)\n",
    "\n",
    "    # Q layer (project down to output_chanels)\n",
    "    self.fc1 = nn.Linear(width,width*2)\n",
    "    self.fc2 = nn.Linear(width*2, self.out_chanels)\n",
    "\n",
    "  def forward(self,x):\n",
    "    #Get the grid information\n",
    "    grid = get_grid(shape = x.shape)\n",
    "\n",
    "    # concat to the input\n",
    "    x = torch.cat((x,grid),dim=-1) # (batch,H,W,chanels)\n",
    "\n",
    "    ##Through P layer:\n",
    "    x = self.fc0(x)\n",
    "\n",
    "    x = x.permute(0,3,1,2) #(batch, chanel , H , W)\n",
    "\n",
    "    ##Through T\n",
    "    #1\n",
    "    x1 = self.conv0(x)\n",
    "    x2 = self.w0(x)\n",
    "    x = x1+x2\n",
    "    x = F.gelu(x)\n",
    "\n",
    "    #2\n",
    "    x1 = self.conv1(x)\n",
    "    x2 = self.w1(x)\n",
    "    x = x1+x2\n",
    "    x = F.gelu(x)\n",
    "\n",
    "    #3\n",
    "    x1 = self.conv2(x)\n",
    "    x2 = self.w2(x)\n",
    "    x = x1+x2\n",
    "    x = F.gelu(x)\n",
    "\n",
    "    #4\n",
    "    x1 = self.conv3(x)\n",
    "    x2 = self.w3(x)\n",
    "    x = x1+x2\n",
    "\n",
    "    #Switch back\n",
    "    x = x.permute(0, 2,3, 1) # (batch,H,W,chanels)\n",
    "\n",
    "    ## Through Q\n",
    "    x = self.fc1(x)\n",
    "    x = F.gelu(x)\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "\n",
    "def get_grid(shape): \n",
    "    batch, H,W = shape[0], shape[1],shape[2]\n",
    "\n",
    "    grid_x = torch.tensor(np.linspace(-1,1,H), dtype= torch.float)\n",
    "    grid_y = torch.tensor(np.linspace(-1,1,W), dtype = torch.float)\n",
    "    \n",
    "    grid_x = grid_x.reshape(1, H, 1, 1).repeat([batch, 1, W, 1])\n",
    "    grid_y = grid_y.reshape(1, 1, W, 1).repeat([batch, H, 1, 1])\n",
    "    grid = torch.cat((grid_x, grid_y), dim =-1).to(device) \n",
    "    return grid \n",
    "\n",
    "print(\"FNO2D class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OESnYwx8-l3q"
   },
   "source": [
    "## 2.Loss functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hXTFStOJQYA_"
   },
   "outputs": [],
   "source": [
    "class LpLoss(object):\n",
    "    \"\"\"\n",
    "    A class to compute the relative L2 loss (data loss).\n",
    "    Calculates: mean( ||y_pred - y_true|| / ||y_true|| )\n",
    "    \"\"\"\n",
    "    def __init__(self, size_average=True, d=2, p=2):\n",
    "        super(LpLoss, self).__init__()\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def cal_data_loss(self, y_pred, y_label):\n",
    "        \"\"\"\n",
    "        Calculates the relative L2 error.\n",
    "        y_pred: (Batch, Space, Time)\n",
    "        y_label: (Batch, Space, Time)\n",
    "        \"\"\"\n",
    "\n",
    "        # torch.norm(..., dim=(1,2)) calculates the L2 norm (length)\n",
    "        # of the error for each sample in the batch across both\n",
    "        # time and space dimensions.\n",
    "        # Shape: (Batch,)\n",
    "        diff_norms = torch.norm(y_pred - y_label, self.p, dim=(1, 2,3))\n",
    "        y_label_norms = torch.norm(y_label, self.p, dim=(1, 2,3))\n",
    "\n",
    "        # Calculate the relative error for each sample\n",
    "        # We add a small epsilon to avoid division by zero\n",
    "        rel_error = diff_norms / (y_label_norms + 1e-7)\n",
    "\n",
    "        if self.size_average:\n",
    "            # Return the average relative error across the batch\n",
    "            return torch.mean(rel_error)\n",
    "        else:\n",
    "            # Return the sum of relative errors\n",
    "            return torch.sum(rel_error)\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.cal_data_loss(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Burger loss fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WC8NrfDqMH6I"
   },
   "outputs": [],
   "source": [
    "## PINO loss function\n",
    "class BurgerLoss(nn.Module):\n",
    "    def __init__(self, t_coord, x_coord, viscosity=0.01, size_average=True):\n",
    "        super(BurgerLoss, self).__init__()\n",
    "\n",
    "        self.t_coord = t_coord\n",
    "        self.x_coord = x_coord\n",
    "        self.size_average = size_average\n",
    "        self.viscosity = viscosity # Store viscosity\n",
    "\n",
    "        # Calculate time step (dt) and space step (dx)\n",
    "        self.dt = (t_coord[1] - t_coord[0]).item()\n",
    "        self.dx = (x_coord[1] - x_coord[0]).item()\n",
    "\n",
    "        print(f\"Physics Loss Initialized:\")\n",
    "        print(f\"  dt = {self.dt:.4f}\")\n",
    "        print(f\"  dx = {self.dx:.4f}\")\n",
    "        print(f\"  viscosity (nu) = {self.viscosity}\")\n",
    "\n",
    "        self.data_loss_fn = LpLoss(self.size_average)\n",
    "\n",
    "    def cal_residual_loss(self, u):\n",
    "        \"\"\"\n",
    "        Calculates the physics residual of the Burgers' equation:\n",
    "        f = u_t + u * u_x - nu * u_xx\n",
    "\n",
    "        We use central differences to approximate the derivatives.\n",
    "        u shape is (Batch, Space, Time), e.g., (32, 1024, 201)\n",
    "        \"\"\"\n",
    "\n",
    "        # --- 1. Calculate du/dt (Time derivative) ---\n",
    "        # Slicing on dim=2 (Time)\n",
    "        # Formula: (f(t+1) - f(t-1)) / (2*dt)\n",
    "        u_t = (u[:, :, 2:] - u[:, :, :-2]) / (2 * self.dt)\n",
    "\n",
    "        # --- 2. Calculate du/dx (Space derivative) ---\n",
    "        # Slicing on dim=1 (Space)\n",
    "        # Formula: (f(x+1) - f(x-1)) / (2*dx)\n",
    "        u_x = (u[:, 2:, :] - u[:, :-2, :]) / (2 * self.dx)\n",
    "\n",
    "        # --- 3. Calculate d2u/dx2 (Second space derivative) ---\n",
    "        # Slicing on dim=1 (Space)\n",
    "        # Formula: (f(x+1) - 2*f(x) + f(x-1)) / (dx^2)\n",
    "        u_xx = (u[:, 2:, :] - 2 * u[:, 1:-1, :] + u[:, :-2, :]) / (self.dx**2)\n",
    "\n",
    "        # --- 4. Align Tensors ---\n",
    "        # All tensors must be on the common \"inner\" grid: (B, 1022, 199)\n",
    "        # We lose 2 points in Space (dim=1) and 2 in Time (dim=2)\n",
    "\n",
    "        # slice both X and T\n",
    "        u_inner = u[:, 1:-1, 1:-1] \n",
    "\n",
    "        # slice only in X\n",
    "        u_t_inner = u_t[:, 1:-1, :]  \n",
    "\n",
    "        # slice only in T\n",
    "        u_x_inner = u_x[:, :, 1:-1]  \n",
    "\n",
    "        # slice only in T\n",
    "        u_xx_inner = u_xx[:, :, 1:-1] \n",
    "\n",
    "        # --- 5. Calculate Residual ---\n",
    "        residual = u_t_inner + u_inner * u_x_inner - self.viscosity * u_xx_inner\n",
    "        return residual\n",
    "\n",
    "    def forward(self, x_in, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Calculates the total loss.\n",
    "        x_in:   Input (t=0). Shape (B,1024,1)\n",
    "        y_pred: Prediction (t=1..200). Shape (B, 1024, 200)\n",
    "        y_true: Ground truth (t=1..200). Shape (B, 1024, 200 )\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Data Loss (L_data)\n",
    "        # This is correct. LpLoss will compare (B, 1024, 200) tensors.\n",
    "        data_loss = self.data_loss_fn(y_pred, y_true)\n",
    "\n",
    "        # 2. Physics Loss (L_physics)\n",
    "\n",
    "        u_full_pred = torch.cat((x_in, y_pred), dim=2) # Shape: (B, 1024, 201)\n",
    "\n",
    "        # Calculate the residual\n",
    "        residual = self.cal_residual_loss(u_full_pred)\n",
    "\n",
    "        # Physics loss is the MSE of the residual\n",
    "        physics_loss = F.mse_loss(residual, torch.zeros_like(residual))\n",
    "\n",
    "        return data_loss, physics_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Loss function for autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLossOnly(nn.Module):\n",
    "    def __init__(self,size_average = True):\n",
    "        super(DataLossOnly,self).__init__()\n",
    "        self.size_average = size_average \n",
    "        self.data_loss_fn = LpLoss(size_average)\n",
    "    def forward(self,x_current,y_pred,y_target): \n",
    "        data_loss = self.data_loss_fn(y_pred,y_target) \n",
    "\n",
    "        return data_loss, torch.tensor(0.0).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwrbOb0Im6_N"
   },
   "source": [
    "## 3 Training & Evaluating process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVCqCrmtnB6c"
   },
   "source": [
    "### 3.1 Training and testing loops + printing time method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UiPrn5AiItIY"
   },
   "outputs": [],
   "source": [
    "# TRAINING LOOP (Physics-Informed Experience Learning - PIEL)\n",
    "def training_loop(model: torch.nn.Module,\n",
    "                  data_loader: torch.utils.data.DataLoader,\n",
    "                  phys_weight:float,\n",
    "                  loss_fn : torch.nn.Module,\n",
    "                  optimizer: torch.optim.Optimizer,\n",
    "                  device,\n",
    "                  noise_level,\n",
    "                  ROLLOUT_STEPS,\n",
    "                  SAMPLING_PROB):\n",
    "    \n",
    "    # --- INITIALIZATION ---\n",
    "    model.train()\n",
    "    \n",
    "    # These track the *total* accumulated loss for reporting over the epoch\n",
    "    total_data_loss_epoch = 0.0\n",
    "    total_phys_loss_epoch = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # --- BATCH ITERATION ---\n",
    "    for batch, (x_batch_in, y_batch_target) in enumerate(data_loader):\n",
    "        x_batch_in, y_batch_target = x_batch_in.to(device), y_batch_target.to(device)\n",
    "        if noise_level >0.0: \n",
    "            noise = torch.randn_like(x_batch_in)*x_batch_in.std() * noise_level \n",
    "            x_batch_in +=  noise\n",
    "        \n",
    "        # Current input window: Anchor the sequence to the initial history\n",
    "        x_current = x_batch_in \n",
    "        loss_accumulate = 0.0\n",
    "\n",
    "        # Check if y_batch has enough steps (safety check, 5 steps needed)\n",
    "        if y_batch_target.size(2) < ROLLOUT_STEPS:\n",
    "             # Skip or handle error if the dataset didn't provide enough steps\n",
    "             print(f\"Warning: Batch {batch} skipped. Expected {ROLLOUT_STEPS} steps, got {y_batch_target.size(2)}.\")\n",
    "             continue\n",
    "\n",
    "        rollout_loss_accumulated =0.0\n",
    "        # --- PIEL ROLLOUT LOOP (Accumulates Loss for BPTT) ---\n",
    "        for step in range(ROLLOUT_STEPS):\n",
    "            \n",
    "            # 1. Forward Pass\n",
    "            y_pred_step = model(x_current)\n",
    "\n",
    "            # 2. Scheduled Sampling (PIEL)\n",
    "            if torch.rand(1) < SAMPLING_PROB:\n",
    "                # Teacher Forcing: Use the ground truth for the next input window\n",
    "                next_input = y_batch_target[:, :,:, step*CHANNELS:(step+1)*CHANNELS]\n",
    "            else:\n",
    "                # Self-Supervision (PIEL): Use the model's prediction for the next input window\n",
    "                next_input = y_pred_step\n",
    "            \n",
    "            # 3. Accumulate Loss for this step\n",
    "            # Ground truth for this step is y_batch_target[:, :,:, step*2:(step+1)*2]\n",
    "            data_loss_step, phys_loss_step = loss_fn(\n",
    "                x_current, \n",
    "                y_pred_step, \n",
    "                y_batch_target[:, :, :,step*CHANNELS:(step+1)*CHANNELS]\n",
    "            )\n",
    "            \n",
    "            # Sum up the weighted loss contributions\n",
    "            rollout_loss_accumulated += data_loss_step + phys_weight * phys_loss_step\n",
    "            \n",
    "            # 4. Advance Input Window (Crucial for BPTT)\n",
    "            # Drop the oldest step (t=0) and add the sampled step (t=11). \n",
    "            # The assignment is necessary to maintain the computational graph.\n",
    "            x_current = torch.cat(\n",
    "                (x_current[:, :, :, CHANNELS:], next_input), \n",
    "                dim=-1\n",
    "            )\n",
    "            \n",
    "            # --- Tracking Metrics for Reporting ---\n",
    "            total_data_loss_epoch += data_loss_step.item()\n",
    "            total_phys_loss_epoch += phys_loss_step.item()\n",
    "            total_samples += 1\n",
    "            \n",
    "        # --- END OF ROLLOUT LOOP ---\n",
    "        \n",
    "        # 5. Backward Pass and Optimization (Done once per batch for the full accumulated rollout loss)\n",
    "        optimizer.zero_grad()\n",
    "        rollout_loss_accumulated.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "    # --- SCALING FOR REPORTING (Average over all steps and batches) ---\n",
    "    train_data_loss = total_data_loss_epoch / total_samples\n",
    "    train_phys_loss = total_phys_loss_epoch / total_samples\n",
    "\n",
    "    return train_data_loss, train_phys_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aMb5BiS5qdhL"
   },
   "outputs": [],
   "source": [
    "#Testing loop\n",
    "def testing_loop(model:torch.nn.Module,\n",
    "                 data_loader: torch.utils.data.DataLoader,\n",
    "                 loss_fn:torch.nn.Module,\n",
    "                 device):\n",
    "\n",
    "  model.eval()\n",
    "  val_data_loss = 0\n",
    "  with torch.inference_mode(): # Disable gradient calculation\n",
    "    for batch,(x_batch, y_batch) in enumerate(data_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        # We only care about the data loss for validation\n",
    "        data_loss, _ = loss_fn(x_batch, y_pred, y_batch[:,:,:,0:2]) #First step\n",
    "\n",
    "        #Accumulating\n",
    "        val_data_loss += data_loss.item()\n",
    "\n",
    "\n",
    "  #Scaling\n",
    "  val_data_loss /= len(data_loader)\n",
    "\n",
    "  return val_data_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TxQddBbytHSx"
   },
   "outputs": [],
   "source": [
    "def printing_time(start, end, model_name):\n",
    "    print(f\"It takes {(end-start):.2f} s to train {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dy8_8IFhyX8A"
   },
   "source": [
    "### 3.2 Traing & validating for epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nZ1R_mhfs4_1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def train_and_test(lr_scheduler, model: torch.nn.Module,\n",
    "                   loss_fn:torch.nn.Module, optimizer: torch.optim.Optimizer,phys_weight :float, train_loader,\n",
    "                   dev_loader, epochs = 100, device = device,noise_level =0.0, rollout_steps =5, sampling_prob =0.9):\n",
    "  #Setups\n",
    "  model.to(device)\n",
    "\n",
    "  results = {\n",
    "      \"train_data_loss\":[],\n",
    "      \"train_phys_loss\":[],\n",
    "      \"val_data_loss\":[],\n",
    "  }\n",
    "\n",
    "  #Start timing\n",
    "  start_time = timer()\n",
    "\n",
    "  #Start\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    if epoch %8 == 0:\n",
    "      print(\"--------------------------\")\n",
    "      print(f\"Epoch: {epoch}\\n--------------\")\n",
    "        \n",
    "      sampling_prob -=0.1\n",
    "\n",
    "    #Training\n",
    "    train_data_loss, train_phys_loss = training_loop(model,train_loader, phys_weight, loss_fn, optimizer,device,noise_level,rollout_steps,sampling_prob)\n",
    "\n",
    "    #Testing\n",
    "    val_data_loss = testing_loop(model,dev_loader,loss_fn,device)\n",
    "\n",
    "    #Decay lr based on val_loss\n",
    "    lr_scheduler.step(val_data_loss)\n",
    "\n",
    "    #Printing\n",
    "    if epoch % 8 ==0:\n",
    "      print(f\"Train data Loss:{train_data_loss:.3f} || Train physic Loss:{train_phys_loss:.3f} || Test Loss:{val_data_loss:.3f}\")\n",
    "\n",
    "    #Restore values\n",
    "    results[\"train_data_loss\"].append(train_data_loss.item() if isinstance(train_data_loss, torch.Tensor) else train_data_loss)\n",
    "    results[\"train_phys_loss\"].append(train_phys_loss.item() if isinstance(train_phys_loss, torch.Tensor) else train_phys_loss)\n",
    "    results[\"val_data_loss\"].append(val_data_loss.item() if isinstance(val_data_loss, torch.Tensor) else val_data_loss)\n",
    "\n",
    "  #Printing time\n",
    "  print(\"--------------------------\")\n",
    "  end_time = timer()\n",
    "  printing_time(start_time,end_time,type(model).__name__)\n",
    "\n",
    "  return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr18PLMh4_QT"
   },
   "source": [
    "### 3.3 Evaluating model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XIm_8vA65C2K"
   },
   "outputs": [],
   "source": [
    "def evaluating(model: torch.nn.Module, loss_fn:torch.nn.Module,\n",
    "               data_loader:torch.utils.data.DataLoader, device =device):\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  test_loss =0\n",
    "\n",
    "  #Evaluating\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for x_batch, y_batch in data_loader:\n",
    "      x_batch , y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "\n",
    "      #Forward\n",
    "      y_pred = model(x_batch)\n",
    "\n",
    "      #Loss and acc\n",
    "      loss,_= loss_fn(x_batch,y_pred,y_batch)\n",
    "      test_loss += loss\n",
    "  print(\"Finished evaluating your model\")\n",
    "\n",
    "  return {\"model\" : model.__class__.__name__,\n",
    "          \"loss\":test_loss.item()/len(data_loader)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTXuX_L7-HcG"
   },
   "source": [
    "# III. Models 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vigQePZJFIG"
   },
   "source": [
    "## 1.DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 T1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfUkEI3mJesF",
    "outputId": "8e879511-23ae-43f3-fdc1-920d27e5bf62"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BurgerDataset_t1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Data for model using 1time step input\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_t1 \u001b[38;5;241m=\u001b[39m \u001b[43mBurgerDataset_t1\u001b[49m(sol_tensor)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Attribute of dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m total_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_t1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BurgerDataset_t1' is not defined"
     ]
    }
   ],
   "source": [
    "#Data for model using 1time step input\n",
    "data_t1 = BurgerDataset_t1(sol_tensor)\n",
    "\n",
    "# Attribute of dataset\n",
    "total_len = len(data_t1)\n",
    "train_size = int(total_len * 0.6 )\n",
    "dev_size = int(total_len *0.2)\n",
    "test_size = total_len - train_size - dev_size\n",
    "BATCH_SIZE = 32\n",
    "print(f\"Number of samples: {total_len}\")\n",
    "\n",
    "#Split datasets\n",
    "train_dataset_t1 , dev_dataset_t1, test_dataset_t1 = random_split(data_t1, [train_size, dev_size, test_size])\n",
    "\n",
    "\n",
    "# Generating loader\n",
    "g = torch.Generator()\n",
    "g.manual_seed(RAND_SEED)\n",
    "\n",
    "train_loader_t1 = DataLoader(\n",
    "    dataset = train_dataset_t1,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle  = True,\n",
    "    num_workers = 0,\n",
    "    generator = g\n",
    ")\n",
    "\n",
    "dev_loader_t1 = DataLoader(\n",
    "    dataset = dev_dataset_t1,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle  = False,\n",
    "    num_workers = 0,\n",
    "    generator = g\n",
    "\n",
    ")\n",
    "\n",
    "test_loader_t1 = DataLoader(\n",
    "    dataset = test_dataset_t1,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle  = False,\n",
    "    num_workers =0,\n",
    "    generator = g\n",
    ")\n",
    "\n",
    "x_z,y_z = next(iter(train_loader_t1))\n",
    "print(x_z.shape)\n",
    "print(y_z.shape)\n",
    "print(\"\\nDataLoaders1t created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 T10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples (windows): 20000\n",
      "Total dev samples (windows): 5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from numpy import random\n",
    "\n",
    "#Attributes of the data set\n",
    "T_in = 10\n",
    "T_out = 5\n",
    "BATCH_SIZE = 32\n",
    "WINDOW_PER_SIM_TRAIN = 20 # 20 random windows per sim\n",
    "WINDOW_PER_SIM_DEV = 50  \n",
    "\n",
    "# --- 2. Create the new DataLoaders ---\n",
    "train_data_t10 = CylinderFlow_Autoregressive_Sparse(train_path, t_in=T_in, t_out=T_out, windows_per_sim=WINDOW_PER_SIM_TRAIN)\n",
    "dev_data_t10 = CylinderFlow_Autoregressive_Sparse(dev_path, t_in=T_in, t_out=T_out, windows_per_sim=WINDOW_PER_SIM_DEV)\n",
    "test_data_t10 = CylinderFlow_Autoregressive_Sparse(test_path, t_in=T_in, t_out=T_out, windows_per_sim=WINDOW_PER_SIM_DEV)\n",
    "\n",
    "print(f\"Total training samples (windows): {len(train_data_t10)}\") # 60,000\n",
    "print(f\"Total dev samples (windows): {len(dev_data_t10)}\")      # 25,000\n",
    "\n",
    "g2 = torch.Generator().manual_seed(RAND_SEED)\n",
    "train_loader_t10 = DataLoader(train_data_t10, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, generator=g2, pin_memory=True)\n",
    "dev_loader_t10 = DataLoader(dev_data_t10, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader_t10 = DataLoader(test_data_t10, batch_size=BATCH_SIZE, shuffle=True, num_workers=0,pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iEmNvrMskFm"
   },
   "source": [
    "## 4. Model 4\n",
    "**(Plautaue at .. epoch after .. hour)\n",
    "\n",
    "** Model 4 (Autoregressive, 3-in) Test Loss: \n",
    "\n",
    "- use data_t10 \n",
    "- FNO2D\n",
    "- DatalossOnly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPER Parameters from the paper\n",
    "WIDTH =64\n",
    "T_IN =10\n",
    "T_OUT =1\n",
    "MODE_X = 12\n",
    "MODE_Y = 12\n",
    "\n",
    "#Define model\n",
    "model4 = FNO2D(in_chanels=T_IN*CHANNELS, \n",
    "                 out_chanels=T_OUT*CHANNELS, \n",
    "                 mode_x=MODE_X, \n",
    "                 mode_y=MODE_Y, \n",
    "                 width=WIDTH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn4 = DataLossOnly(size_average =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#Optimizer (Adam)\n",
    "LR =0.001\n",
    "optimizer4 = torch.optim.Adam(model4.parameters(),lr = LR)\n",
    "\n",
    "# We'll use 'ReduceLROnPlateau'.\n",
    "# This scheduler will automatically \"reduce\" the learning rate\n",
    "# when it detects that our validation loss has \"plateaued\" (stopped improving).\n",
    "scheduler4 = ReduceLROnPlateau(\n",
    "    optimizer4,\n",
    "    mode='min',      # It will look at the validation loss, where 'min' is better\n",
    "    factor=0.1,      # Reduce LR by a factor of 10 (e.g., 0.001 -> 0.0001)\n",
    "    patience=5,      # Wait 5 epochs for improvement before reducing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training and testing model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e51ad9d8fe84407a8fbcfe94d275fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Epoch: 0\n",
      "--------------\n",
      "Train data Loss:0.056 || Train physic Loss:0.000 || Test Loss:0.028\n",
      "--------------------------\n",
      "Epoch: 8\n",
      "--------------\n",
      "Train data Loss:0.014 || Train physic Loss:0.000 || Test Loss:0.009\n",
      "--------------------------\n",
      "It takes 3783.50 s to train FNO2D\n"
     ]
    }
   ],
   "source": [
    "# --- HYPERPARAMETERS ---\n",
    "# ROLLOUT_STEPS must match the number of output steps provided by your DataLoader's y_batch.\n",
    "\n",
    "ROLLOUT_STEPS = 5  \n",
    "SAMPLING_PROB = 0.9 \n",
    "EPOCHS =16\n",
    "NOISE_LEVEL =0.05\n",
    "PHYS_WEIGHT =0.0\n",
    "\n",
    "result4 = train_and_test(scheduler4, model = model4,\n",
    "              loss_fn = loss_fn4, optimizer = optimizer4,phys_weight = PHYS_WEIGHT, train_loader = train_loader_t10,\n",
    "              dev_loader = dev_loader_t10, epochs = EPOCHS,device = device, noise_level = NOISE_LEVEL, rollout_steps =ROLLOUT_STEPS , sampling_prob =SAMPLING_PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 4: {'train_data_loss': [0.05087211075901985, 0.019582114146053792, 0.01718435420244932, 0.01622707962244749, 0.013885308791100979, 0.012675398913770914, 0.014305818380713462, 0.01390168257534504, 0.013852717347741126, 0.010688792301565409, 0.003958933025971055, 0.003676559438854456, 0.0035291603688895703, 0.0034527564647048713, 0.0033684210056811573, 0.003355002436041832], 'train_phys_loss': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_data_loss': [0.014770651550572009, 0.011961442513308327, 0.01349966850274118, 0.008752051269291503, 0.02810782082616144, 0.013955968230440739, 0.02735861696919818, 0.013814288179633344, 0.01048888521755387, 0.009351865329133098, 0.003875018720319317, 0.0037171958306519565, 0.0036459622057866616, 0.0034536947466575416, 0.0033496209067310307, 0.003377579756164152]}\n"
     ]
    }
   ],
   "source": [
    "    print(f\"Result 4: {result4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving and comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szrIoqFiriPS",
    "outputId": "aec6d1c2-c7f4-4027-827f-aaa8492b9c24"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"Saving initial untrained models...\")\n",
    "\n",
    "# 1. Define a path to save your models in your Drive\n",
    "MODELS_PATH = Path(\"models\")\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "# 2. Define the save paths for each model\n",
    "MODEL4_SAVE_PATH = MODELS_PATH / \"model4_initial_state.pth\"\n",
    "\n",
    "\n",
    "# 3. Save the models' state dictionaries\n",
    "# .state_dict() saves only the learnable parameters (weights and biases)\n",
    "try:\n",
    "    torch.save(model4.state_dict(), MODEL4_SAVE_PATH)\n",
    "\n",
    "    print(f\"Successfully saved model4 to: {MODEL4_SAVE_PATH}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving models: {e}\")\n",
    "\n",
    "# To load them back later (for inference or to resume training), you would:\n",
    "# 1. Re-create the model instance: model0 = FNO1D(WIDTH, K_MAX, x_grid_tensor).to(device)\n",
    "# 2. Load the weights: model0.load_state_dict(torch.load(MODEL0_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_autoregressive_rollout (2D version) function defined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# This must match your model and dataset\n",
    "GRID_SIZE = 64\n",
    "CHANNELS = 2 # vx, vy\n",
    "T_IN = 10    # 10 steps in\n",
    "\n",
    "def evaluate_autoregressive_rollout(model, dataset, loss_fn,t_in, device):\n",
    "    \"\"\"\n",
    "    Performs a full autoregressive rollout on the 2D grid dataset\n",
    "    by loading each sample file.\n",
    "    \n",
    "    Args:\n",
    "        model: Your trained 2D FNO model\n",
    "        dataset: The CylinderFlowGridDataset object (e.g., dev_dataset)\n",
    "        loss_fn: An instance of your 2D LpLoss (NOT DataLossOnly)\n",
    "        device: Your \"cuda\" or \"cpu\" device\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_rollout_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Loop through each sample file in the dataset\n",
    "        for i in tqdm(range(len(dataset.file_paths)), desc=\"Evaluating Rollout\"):\n",
    "            \n",
    "            # 1. Load the full 600-step trajectory for one sample\n",
    "            data = torch.load(dataset.file_paths[i])\n",
    "            # Shape: (600, 64, 64, 2)\n",
    "            velocity_traj = data['velocity'].to(device).to(torch.float32)\n",
    "\n",
    "            # --- 2. Create the first input window ---\n",
    "            # Get t=0...9\n",
    "            x_window_t = velocity_traj[:t_in] # (10, 64, 64, 2)\n",
    "            \n",
    "            # Reshape to (1, 64, 64, 20) for the model\n",
    "            current_window = x_window_t.permute(1, 2, 0, 3) \n",
    "            current_window = current_window.reshape(GRID_SIZE, GRID_SIZE, t_in * CHANNELS)\n",
    "            current_window = current_window.unsqueeze(0) # Add batch dim\n",
    "\n",
    "            # --- 3. Create the ground truth for comparison ---\n",
    "            # Get t=10...599\n",
    "            y_true_all_steps = velocity_traj[t_in:] # (590, 64, 64, 2)\n",
    "            \n",
    "            # Reshape to (1, 64, 64, 1180)\n",
    "            y_true_rollout = y_true_all_steps.permute(1, 2, 0, 3)\n",
    "            y_true_rollout = y_true_rollout.reshape(GRID_SIZE, GRID_SIZE, -1)\n",
    "            y_true_rollout = y_true_rollout.unsqueeze(0)\n",
    "\n",
    "            \n",
    "            predictions = [] # List to store all 590 predictions\n",
    "            n_steps = y_true_all_steps.shape[0] # 590 steps\n",
    "            \n",
    "            # --- 4. Start the rollout loop ---\n",
    "            for _ in range(n_steps):\n",
    "                # model input: (1, 64, 64, 20)\n",
    "                # model output: (1, 64, 64, 2)\n",
    "                y_pred_step = model(current_window)\n",
    "                \n",
    "                if torch.any(torch.isinf(y_pred_step)) or torch.any(torch.isnan(y_pred_step)):\n",
    "                    print(f\"!!! Rollout became unstable at step {i} !!!\")\n",
    "                    break \n",
    "                \n",
    "                predictions.append(y_pred_step)\n",
    "                \n",
    "                # --- 5. Feed back in ---\n",
    "                # Drop oldest 2 channels, add new 2 channels\n",
    "                current_window = torch.cat(\n",
    "                    (current_window[:, :, :, CHANNELS:], y_pred_step), \n",
    "                    dim=3 # Concatenate on the channel dimension\n",
    "                )\n",
    "            \n",
    "            # --- 6. Stack all predictions ---\n",
    "            # y_pred_full_rollout shape: (1, 64, 64, 1180)\n",
    "            if len(predictions) != n_steps: # Handle unstable rollout\n",
    "                print(\"Rollout failed, skipping this sample.\")\n",
    "                continue\n",
    "                \n",
    "            y_pred_full_rollout = torch.cat(predictions, dim=3)\n",
    "            \n",
    "            # --- 7. Calculate final loss ---\n",
    "            # Use the simple LpLoss, not the DataLossOnly wrapper\n",
    "            rollout_loss = loss_fn(x_window_t,y_pred_full_rollout, y_true_rollout)\n",
    "            total_rollout_loss += rollout_loss[0]\n",
    "\n",
    "    # Return the average loss over all test samples\n",
    "    return total_rollout_loss / len(dataset.file_paths)\n",
    "\n",
    "print(\"evaluate_autoregressive_rollout (2D version) function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FAIR comparison rollout for Model 2 and 3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e889ce0fd640d6b61ec00210b0d8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Rollout:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fair Comparison Test Results ---\n",
      "Model 0 (Direct, Data-Only) Test Loss:     0.0617\n",
      "Model 1 (Direct, PINO, w=0.1) Test Loss:   0.063\n",
      "Model 4 (Autoregressive, 3-in) Test Loss: 0.3364\n"
     ]
    }
   ],
   "source": [
    "# (Run this in a new cell after training model2)\n",
    "\n",
    "print(\"Starting FAIR comparison rollout for Model 2 and 3...\")\n",
    "\n",
    "# Use test_loader_t1 (from cell 17) for a fair test\n",
    "# Use loss_fn0 (from cell 55) for a fair data-only loss\n",
    "\n",
    "rollout_loss_m4 = evaluate_autoregressive_rollout(\n",
    "    model=model4, \n",
    "    dataset=test_data_t10, \n",
    "    loss_fn=loss_fn4,           \n",
    "    device=device,\n",
    "    t_in=10# <-- This correctly matches your model2\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fair Comparison Test Results ---\")\n",
    "print(f\"Model 0 (Direct, Data-Only) Test Loss:     0.0617\") # From cell 105\n",
    "print(f\"Model 1 (Direct, PINO, w=0.1) Test Loss:   0.063\") # From cell 111\n",
    "print(f\"Model 4 (Autoregressive, 3-in) Test Loss: {rollout_loss_m4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "ZgPw4e7GX44c",
    "outputId": "45db1610-5818-476e-f1d2-b39a03706b99"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from results import result1, result0\n",
    "\n",
    "# --- IMPORTANT ---\n",
    "# You must copy the dictionary outputs from your notebook cells\n",
    "# (cell 50 for model0 and cell 67 for model1) and paste them here.\n",
    "# I have used your completed output from cell 50 as an example.\n",
    "# You will need to replace results_1 with your full 30-epoch output.\n",
    "\n",
    "# --- Create Epoch Array ---\n",
    "# Assumes both models were trained for the same number of epochs\n",
    "result0 = {'train_data_loss': [0.4110739827156067, 0.23925882577896118, 0.20458190143108368, 0.19567212462425232, 0.17208527028560638, 0.16243018209934235, 0.16161102056503296, 0.1497381031513214, 0.1424730122089386, 0.13952957093715668, 0.13167569041252136, 0.1314629167318344, 0.13347485661506653, 0.1252565085887909, 0.12282412499189377, 0.12299676984548569, 0.11490682512521744, 0.11774254590272903, 0.10972394794225693, 0.10895457118749619, 0.11255241930484772, 0.10872527211904526, 0.10500839352607727, 0.10531488806009293, 0.10197656601667404, 0.10225849598646164, 0.10453744977712631, 0.10029015690088272, 0.09715524315834045, 0.10300806164741516, 0.09627469629049301, 0.09561239928007126, 0.09352393448352814, 0.09420380741357803, 0.09108910709619522, 0.08958891034126282, 0.08952014893293381, 0.0945991724729538, 0.08834867179393768, 0.08973327279090881, 0.08752944320440292, 0.08816472440958023, 0.08264376223087311, 0.08628880232572556, 0.08571340143680573, 0.08524730801582336, 0.08302949368953705, 0.08613189309835434, 0.08262985199689865, 0.0860193744301796, 0.08372187614440918, 0.08175082504749298, 0.0784018486738205, 0.07959341257810593, 0.0778963565826416, 0.0784391239285469, 0.07724633812904358, 0.07945655286312103, 0.07577656954526901, 0.08160007745027542, 0.08002155274152756, 0.07753711193799973, 0.07859975844621658, 0.0764986202120781, 0.07406435161828995, 0.0723976269364357, 0.0739876925945282, 0.07461126893758774, 0.07415719330310822, 0.07250193506479263, 0.07158296555280685, 0.0725533589720726, 0.07088251411914825, 0.0713513195514679, 0.07630784064531326, 0.07213081419467926, 0.06057237461209297, 0.05909671634435654, 0.05885971337556839, 0.058606330305337906, 0.05847145989537239, 0.058534953743219376, 0.05826769769191742, 0.05820904299616814, 0.05816391482949257, 0.05802098661661148, 0.058009110391139984, 0.057899314910173416, 0.0577441044151783, 0.05807175859808922, 0.057901881635189056, 0.05766398832201958, 0.05750136449933052, 0.05765574425458908, 0.05767909809947014, 0.05718502774834633, 0.0571594312787056, 0.057440076023340225, 0.057252444326877594, 0.05690488591790199, 0.05690895393490791, 0.05691118910908699, 0.056899115443229675, 0.05672742426395416, 0.05663994699716568, 0.05657578259706497, 0.05658656731247902, 0.05656237527728081, 0.05650690570473671, 0.05641086399555206, 0.056626658886671066, 0.05617628991603851, 0.05610703304409981, 0.05624675750732422, 0.0564168281853199, 0.05596563592553139, 0.05595419928431511, 0.055922769010066986, 0.05574888736009598, 0.05567246302962303, 0.05585700646042824, 0.055710289627313614, 0.05597971752285957, 0.05580592527985573, 0.05584094300866127, 0.05574943125247955, 0.05546464025974274, 0.055211909115314484, 0.05526223033666611, 0.05522475764155388, 0.05564416944980621, 0.05541204661130905, 0.05530177429318428, 0.05499405786395073, 0.05497822165489197, 0.055076297372579575, 0.05496148020029068, 0.0547984354197979, 0.05514082685112953, 0.055032216012477875, 0.05497255548834801, 0.05481015145778656, 0.05463377758860588, 0.054746970534324646, 0.05454118177294731, 0.05435951054096222, 0.05476135388016701, 0.054547540843486786, 0.05460606887936592, 0.05433722585439682], 'train_phys_loss': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_data_loss': [0.2572142621354451, 0.21145666733620658, 0.19115776675088064, 0.1802690809681302, 0.17118003964424133, 0.15824853404173775, 0.17612133447139983, 0.14190959067098677, 0.13687605919345977, 0.1335370416442553, 0.13127026198402283, 0.1297357457261237, 0.12929621764591762, 0.12160444259643555, 0.15860963624621194, 0.12193896668770957, 0.13977357768823231, 0.11167937008634446, 0.11051414717757513, 0.11135667432395238, 0.11710662323804129, 0.10518188218748759, 0.10574372337451057, 0.11249114868659822, 0.10047837618797545, 0.10142433477772607, 0.09888798826270634, 0.09998914327413316, 0.10522396519543632, 0.10073239829332109, 0.09524016141418427, 0.09673324570296303, 0.0917700073785252, 0.09177173449406548, 0.09537116840245231, 0.0898829839295811, 0.09122747289282936, 0.09619156460440348, 0.09295563683623359, 0.09442581981420517, 0.0865695761546256, 0.08646621212126716, 0.08684119321997204, 0.089255215156646, 0.0872151656519799, 0.08562316390730086, 0.0906154742789647, 0.08481654382887341, 0.08532685396217164, 0.09498826461651969, 0.08751835993358068, 0.08428332919166201, 0.10068383375330577, 0.08187451710303624, 0.08577221821224879, 0.07869435054442239, 0.08390380430316167, 0.08048265462829954, 0.07998732867695037, 0.07940924309548877, 0.08420824247693258, 0.07788392381062584, 0.08739961293481645, 0.07972035036673622, 0.07721601213727679, 0.07708905519001068, 0.08067990259991752, 0.07722300066361351, 0.09537790822131294, 0.07313832954045325, 0.07719190785336116, 0.0766766376438595, 0.07478425805530851, 0.0733860337308475, 0.07394332036612526, 0.07324050420096942, 0.06501247840268272, 0.06459305080629531, 0.06465581023976916, 0.06449638975281564, 0.06492461772665145, 0.06398267458592143, 0.0644972435538731, 0.06389726597874884, 0.06378511274381289, 0.06382066389871022, 0.06389406625004042, 0.06369595667199483, 0.06350627009357725, 0.0635057757534678, 0.06327962041610763, 0.06329736746256313, 0.06380144559911319, 0.06318371796182223, 0.06303894472500635, 0.06305838967599565, 0.06306172854134015, 0.06316362587468964, 0.06280702772358107, 0.06297555837839369, 0.06427314351238901, 0.06272708405814474, 0.06253687392861124, 0.06268517643449799, 0.062455795646186855, 0.06254122944341765, 0.06238306195489944, 0.06241820912275996, 0.062523636493891, 0.06233104856477843, 0.0623762799752137, 0.06204700215704857, 0.06217261888678112, 0.06274812715867209, 0.0618926150694726, 0.06204395309563667, 0.06187254799500344, 0.061938941537860844, 0.06266835385135242, 0.06309285830883753, 0.06154843940148278, 0.06175081975876339, 0.06152673846199399, 0.06216449757653569, 0.06183923437954888, 0.06193630456451386, 0.06164784404256987, 0.06338022170322281, 0.06135745565333064, 0.06773435566869992, 0.06207913467808375, 0.0613168551926575, 0.061097964880958436, 0.06094158074212453, 0.06099840265417856, 0.06099581180347337, 0.06111503788639629, 0.06091841511310093, 0.06254472116392756, 0.06069481703970167, 0.060798205198749664, 0.0615965946917496, 0.060730867383499, 0.06066012004065135, 0.06074823173029082, 0.0605203460842844, 0.060685241033160496, 0.06406244617842492, 0.06041179910775215, 0.06047245848273474]}\n",
    "\n",
    "epochs = range(len(result0['train_data_loss']))\n",
    "\n",
    "\n",
    "# --- Create Plots ---\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Training Data Loss Comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, result0['train_data_loss'], label='Model 0 (Data Only)', color='blue')\n",
    "plt.plot(epochs, result1['train_data_loss'], label='Model 1 (PINO)', color='red')\n",
    "plt.title('Training Data Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Relative L2 Loss (L_data)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.yscale('log') # Use log scale if losses are very different\n",
    "\n",
    "# Plot 2: Validation Data Loss Comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, result0['val_data_loss'], label='Model 0 (Data Only)', color='blue')\n",
    "plt.plot(epochs, result1['val_data_loss'], label='Model 1 (PINO)', color='red')\n",
    "plt.title('Validation Data Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Relative L2 Loss (L_data)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 3: Model 1 (PINO) Loss Breakdown\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, result1['train_data_loss'], label='Data Loss', color='red')\n",
    "plt.plot(epochs, result1['train_phys_loss'], label='Physics Loss', color='green')\n",
    "plt.title('Model 1 (PINO) Loss Components')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.yscale('log') # Log scale is essential here since physics loss is large\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a98920ebca0440a870a60a7600ef050": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f826a8c1e04462f9f2a3b14ba2c9c7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a47940e2300a41f9960234ae87cc9017",
      "max": 150,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8130aea889c94b018434b8640338c440",
      "value": 150
     }
    },
    "206ade5cd57243c59d1ae55cd5fb498e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cf95d50e8014c0bb6462432fc8f7bc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48fe616a363746f5822fd7350557bcf7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b9eb26f841b45a5a94f43f4e53a4cfb",
      "placeholder": "",
      "style": "IPY_MODEL_5d11d96b53d14e8f8817cf18898e04c1",
      "value": "100%"
     }
    },
    "5403a181ccf54280a9529c6c2aa0d93b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e04cb363eb724b709b61735d95aef361",
      "placeholder": "",
      "style": "IPY_MODEL_a11267c6534149a5959835643a7b1374",
      "value": "100%"
     }
    },
    "54c4444c2e92457e99c685a22247e940": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48fe616a363746f5822fd7350557bcf7",
       "IPY_MODEL_1f826a8c1e04462f9f2a3b14ba2c9c7a",
       "IPY_MODEL_60e161d1266c468e9efa47f42834ee29"
      ],
      "layout": "IPY_MODEL_aeaaa8c959114f1489681efaf449461d"
     }
    },
    "55a1f2e4a9d24d7eb0b71a7aef6116fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb44859138d1400a8ff8ee5f5afc2f95",
      "placeholder": "",
      "style": "IPY_MODEL_ab1d2926c70b45d790e5c40e3cb0950d",
      "value": "150/150[29:45&lt;00:00,11.97s/it]"
     }
    },
    "5d11d96b53d14e8f8817cf18898e04c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60e161d1266c468e9efa47f42834ee29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_206ade5cd57243c59d1ae55cd5fb498e",
      "placeholder": "",
      "style": "IPY_MODEL_1a98920ebca0440a870a60a7600ef050",
      "value": "150/150[29:39&lt;00:00,11.93s/it]"
     }
    },
    "6b9eb26f841b45a5a94f43f4e53a4cfb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8130aea889c94b018434b8640338c440": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8743875a3c56434c94ee74b9bcb21a9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5403a181ccf54280a9529c6c2aa0d93b",
       "IPY_MODEL_89bc54136b924aafa9206a2fb788eb7b",
       "IPY_MODEL_55a1f2e4a9d24d7eb0b71a7aef6116fd"
      ],
      "layout": "IPY_MODEL_a6ac3e77e78d4df1b76738eb81ad0a8d"
     }
    },
    "89bc54136b924aafa9206a2fb788eb7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9403c207ea774b00ace3374512205df9",
      "max": 150,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3cf95d50e8014c0bb6462432fc8f7bc9",
      "value": 150
     }
    },
    "9403c207ea774b00ace3374512205df9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a11267c6534149a5959835643a7b1374": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a47940e2300a41f9960234ae87cc9017": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6ac3e77e78d4df1b76738eb81ad0a8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab1d2926c70b45d790e5c40e3cb0950d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aeaaa8c959114f1489681efaf449461d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb44859138d1400a8ff8ee5f5afc2f95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e04cb363eb724b709b61735d95aef361": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
